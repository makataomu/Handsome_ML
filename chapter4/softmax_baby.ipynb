{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x_instance, class_weights):\n",
    "    return np.dot(x_instance, class_weights)\n",
    "\n",
    "#  true_labes is 0 and 1. 1 if class belongs\n",
    "# loss = 0\n",
    "# for x_instance in X:\n",
    "#     all_scores = [score(x_instance, class_weights) for class_weights in all_weights]\n",
    "#     exp_scores = [exp(class_score) for class_score in all_scores]\n",
    "#     sum_of_exp_scores = sum(exp_scores)\n",
    "#     probs = [exp_score / sum_of_exp_scores for exp_scores in exp_scores]\n",
    "\n",
    "#     true labels and probs should be of the same order\n",
    "#     cross_entropies = true_labels[c] * probs[c] for c in range(len(true_labels))\n",
    "#     loss += sum(cross_entropies)\n",
    "# loss = loss / X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    all_scores = score(X, weights.T)\n",
    "    exp_scores = np.exp(all_scores)\n",
    "    exp_sum_scores = np.sum(exp_scores, axis=1)\n",
    "    exp_sum_scores = exp_sum_scores[:, np.newaxis]\n",
    "\n",
    "    return exp_scores / exp_sum_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_proba(X, weights):\n",
    "#     all_scores = [score(X, class_weights) for class_weights in weights]\n",
    "#     exp_scores = [np.exp(class_score) for class_score in all_scores]\n",
    "#     sum_of_exp_scores = sum(exp_scores)\n",
    "#     print(sum_of_exp_scores)\n",
    "#     probs = [exp_score / sum_of_exp_scores for exp_score in exp_scores]\n",
    "\n",
    "#     return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_val = list(range(10))\n",
    "neg_val = list(range(10, 20))\n",
    "data = {}\n",
    "y = []\n",
    "\n",
    "for i in range(20):\n",
    "    if i % 2 == 0:\n",
    "        data[i] = pos_val\n",
    "        y.append(0)\n",
    "    else:\n",
    "        data[i] = neg_val\n",
    "        y.append(1)\n",
    "\n",
    "X = pd.DataFrame(data).T.to_numpy()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class SoftmaxGD(BaseEstimator, ClassifierMixin):\n",
    "    weights = None\n",
    "\n",
    "    def __init__(self, n_iter = 1, learning_rate = 0.1, warm_start = True) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warm_start = warm_start\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        all_scores = np.dot(X, self.weights.T)\n",
    "        exp_scores = np.exp(all_scores)\n",
    "        exp_sum_scores = np.sum(exp_scores, axis=1)\n",
    "        exp_sum_scores = exp_sum_scores[:, np.newaxis]\n",
    "\n",
    "        return exp_scores / exp_sum_scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.weights is None or self.warm_start is False:\n",
    "            self.classes_ = np.unique(y)  # Use sklearn convention for class labels\n",
    "            self.true_labels = np.zeros((X.shape[0], len(self.classes_)))\n",
    "            self.true_labels[np.arange(X.shape[0]), y] = 1\n",
    "            self.weights = np.random.randn(len(self.classes_), X.shape[1])\n",
    "\n",
    "        for iter in range(self.n_iter):\n",
    "            gradients = np.array([np.zeros(X.shape[1]) for c in self.classes_])\n",
    "\n",
    "            probs = self.predict_proba(X)\n",
    "            all_errors = (probs - self.true_labels).T\n",
    "            for c, errors_class in enumerate(all_errors):\n",
    "                # mult each row by error \n",
    "                result = (X.T * errors_class).T\n",
    "                gradients[c] = sum(result) / X.shape[0]\n",
    "\n",
    "            self.weights = self.weights - self.learning_rate * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class SoftmaxGD(BaseEstimator, ClassifierMixin):\n",
    "    weights = None\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_iter=1, \n",
    "        learning_rate=0.1, \n",
    "        warm_start=True, \n",
    "        early_stopping=False, \n",
    "        tol=1e-4, \n",
    "        patience=10,\n",
    "        alpha=None,\n",
    "        reg_type=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_iter: Number of iterations per epoch.\n",
    "        - learning_rate: Step size for gradient descent.\n",
    "        - warm_start: If True, reuse the weights learned from the previous fit.\n",
    "        - early_stopping: If True, stop training early when validation error stops improving.\n",
    "        - tol: Tolerance for validation error improvement.\n",
    "        - patience: Number of epochs with no improvement before stopping.\n",
    "        - alpha: more alpha less jggly wggly\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warm_start = warm_start\n",
    "        self.early_stopping = early_stopping\n",
    "        self.tol = tol\n",
    "        self.patience = patience\n",
    "        self.alpha = alpha\n",
    "        self.reg_type = reg_type\n",
    "        self.best_weights_ = None\n",
    "\n",
    "    def get_reg_term_for_grad(self, type=\"l1\", alpha=0.1):\n",
    "        if type == \"l1\":\n",
    "            # d (abs(w)) / dw = np.sign(w) are signs (-1/1) \n",
    "            return alpha * np.sign(self.weights)\n",
    "        elif type == \"l2\":\n",
    "            # alpha на веса прав класса\n",
    "            return alpha * self.weights\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        all_scores = np.dot(X, self.weights.T)\n",
    "        exp_scores = np.exp(all_scores)\n",
    "        exp_sum_scores = np.sum(exp_scores, axis=1)\n",
    "        exp_sum_scores = exp_sum_scores[:, np.newaxis]\n",
    "\n",
    "        return exp_scores / exp_sum_scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Training data.\n",
    "        - y: Target labels for training data.\n",
    "        - X_val: Optional, validation data to use for early stopping.\n",
    "        - y_val: Optional, validation labels to use for early stopping.\n",
    "        \"\"\"\n",
    "        if self.weights is None or not self.warm_start:\n",
    "            self.classes_ = np.unique(y)  # Use sklearn convention for class labels\n",
    "            self.true_labels = np.zeros((X.shape[0], len(self.classes_)))\n",
    "            self.true_labels[np.arange(X.shape[0]), y] = 1\n",
    "            self.weights = np.random.randn(len(self.classes_), X.shape[1])\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_val_error = float(\"inf\")\n",
    "        epochs_no_improvement = 0\n",
    "\n",
    "        for epoch in range(self.n_iter):\n",
    "            gradients = np.zeros((len(self.classes_), X.shape[1]))\n",
    "\n",
    "            probs = self.predict_proba(X)\n",
    "            all_errors = (probs - self.true_labels).T\n",
    "            for c, errors_class in enumerate(all_errors):\n",
    "                result = (X.T * errors_class).T\n",
    "                gradients[c] = np.sum(result, axis=0) / X.shape[0] \n",
    "\n",
    "            reg_term = self.get_reg_term_for_grad(alpha=self.alpha, type=self.reg_type)\n",
    "\n",
    "            self.weights = self.weights - self.learning_rate * (gradients + reg_term)\n",
    "\n",
    "            if self.early_stopping and X_val is not None and y_val is not None:\n",
    "                # Validation error calculation\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_error = root_mean_squared_error(y_val, y_val_pred) ** 2\n",
    "\n",
    "                # Early stopping logic\n",
    "                if val_error < best_val_error - self.tol:\n",
    "                    best_val_error = val_error\n",
    "                    self.best_weights_ = self.weights.copy() \n",
    "                    epochs_no_improvement = 0  # Reset patience counter\n",
    "                else:\n",
    "                    epochs_no_improvement += 1\n",
    "\n",
    "                if epochs_no_improvement >= self.patience:\n",
    "                    print(f\"Stopping early at epoch {epoch}. No improvement after {self.patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "        if self.early_stopping and self.best_weights_ is not None:\n",
    "            self.weights = self.best_weights_  \n",
    "\n",
    "        return self  # Return self for compatibility with scikit-learn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 203. No improvement after 200 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\77019\\Desktop\\code\\ml_handson\\Handsome_ML\\env_name\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SoftmaxGD</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (Softmax)</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Accuracy\n",
       "0                      SoftmaxGD  0.916667\n",
       "1  Logistic Regression (Softmax)  0.958333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and train the original SoftmaxGD\n",
    "sgd_reg = SoftmaxGD(n_iter=1000, learning_rate=0.1, warm_start=True, early_stopping=True, tol=1e-7, patience=200, alpha=0.01, reg_type='l2')\n",
    "sgd_reg.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "# Predictions with SoftmaxGD\n",
    "y_pred_sgd = sgd_reg.predict(X_val)\n",
    "\n",
    "# Accuracy of SoftmaxGD\n",
    "sgd_accuracy = accuracy_score(y_val, y_pred_sgd)\n",
    "\n",
    "# Now, let's use scikit-learn's Logistic Regression (which uses softmax for multi-class classification)\n",
    "log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions with Logistic Regression\n",
    "y_pred_logreg = log_reg.predict(X_val)\n",
    "\n",
    "# Accuracy of Logistic Regression\n",
    "logreg_accuracy = accuracy_score(y_val, y_pred_logreg)\n",
    "\n",
    "# Display the comparison of accuracies\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"SoftmaxGD\", \"Logistic Regression (Softmax)\"],\n",
    "    \"Accuracy\": [sgd_accuracy, logreg_accuracy]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SoftmaxGD</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (Softmax)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Accuracy\n",
       "0                      SoftmaxGD  0.966667\n",
       "1  Logistic Regression (Softmax)  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_sgd = sgd_reg.predict(X_test)\n",
    "\n",
    "# Accuracy of SoftmaxGD\n",
    "sgd_accuracy = accuracy_score(y_test, y_pred_sgd)\n",
    "\n",
    "# Predictions with Logistic Regression\n",
    "y_pred_logreg = log_reg.predict(X_test)\n",
    "\n",
    "# Accuracy of Logistic Regression\n",
    "logreg_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Display the comparison of accuracies\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"SoftmaxGD\", \"Logistic Regression (Softmax)\"],\n",
    "    \"Accuracy\": [sgd_accuracy, logreg_accuracy]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
